{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](img/330-banner.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Lecture 20: Class demo\n",
    "\n",
    "UBC 2024-25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/time-series-dall-e.png)\n",
    "\n",
    "The picture is created using DALL-E! \n",
    "\n",
    "> This illustration shows a cityscape with skyscrapers, each featuring digital billboards displaying time-series graphs. These graphs represent various types of data such as stock market trends, weather patterns, and population growth, set against the backdrop of a bustling city with people of diverse descents and genders. This setting should provide a compelling and relatable context for your introduction to time-series lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import (\n",
    "    TimeSeriesSplit,\n",
    "    cross_val_score,\n",
    "    cross_validate,\n",
    "    train_test_split,\n",
    ")\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler\n",
    "\n",
    "plt.rcParams[\"font.size\"] = 12\n",
    "from datetime import datetime\n",
    "\n",
    "DATA_DIR = os.path.join(\"data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Motivation\n",
    "\n",
    "- **Time series** is a collection of data points indexed in time order. \n",
    "- Time series is everywhere:\n",
    "    - Physical sciences (e.g., weather forecasting)  \n",
    "    - Economics, finance (e.g., stocks, market trends)\n",
    "    - Engineering (e.g., energy consumption)\n",
    "    - Social sciences \n",
    "    - Sports analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's start with a simple example from [Introduction to Machine Learning with Python  book](https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/). \n",
    "\n",
    "In New York city there is a network of bike rental stations with a subscription system. The stations are all around the city. The anonymized data is available [here](https://ride.citibikenyc.com/system-data).\n",
    "\n",
    "We will focus on the task is predicting how many people will rent a bicycle from a particular station for a given time and day. We might be interested in knowing this so that we know whether there will be any bikes left at the station for a particular day and time.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import mglearn\n",
    "\n",
    "citibike = mglearn.datasets.load_citibike()\n",
    "citibike.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- The only feature we have is the date time feature. \n",
    "    - Example: 2015-08-01 00:00:00\n",
    "- What's the duration of the intervals? \n",
    "    - The data is collected at regular intervals (every three hours) \n",
    "- The target is the number of rentals in the next 3 hours. \n",
    "    - Example: 3 rentals between 2015-08-01 00:00:00 and 2015-08-01 03:00:00  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We have a time-ordered sequence of data points.\n",
    "- This type of data is distinctive because it is inherently sequential, with an intrinsic order based on time.\n",
    "- The number of bikes available at a station at one point in time is often related to the number of bikes at earlier times. \n",
    "- This is a **time-series forecasting** problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the time duration of our data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "citibike.index.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "citibike.index.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have data for August 2015. Let's visualize our data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 3))\n",
    "xticks = pd.date_range(start=citibike.index.min(), end=citibike.index.max(), freq=\"D\")\n",
    "plt.xticks(xticks, xticks.strftime(\"%a %m-%d\"), rotation=90, ha=\"left\")\n",
    "plt.plot(citibike, linewidth=1)\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Rentals\");\n",
    "plt.title(\"Number of bike rentals over time for a selected bike station\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We see the day and night pattern\n",
    "- We see the weekend and weekday pattern "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Questions you might want to answer: How many people are likely to rent a bike at this station in the next three hours given everything we know about rentals in the past? \n",
    "- We want to learn from the past and predict the future. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ❓❓ Questions for you\n",
    "\n",
    "- Can we use our usual supervised machine learning methodology to predict the number of people who will rent bicycles from a specific station at a given time and date?\n",
    "- How can you adapt traditional machine learning methods for time series forecasting?\n",
    "\n",
    "\n",
    "<br><br><br><br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Train/test split for temporal data\n",
    "\n",
    "- To evaluate the model's ability to generalize, we will divide the data into training and testing subsets.\n",
    "- What will happen if we split this data the usual way?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(citibike, test_size=0.2, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 3))\n",
    "train_df_sort = train_df.sort_index()\n",
    "test_df_sort = test_df.sort_index()\n",
    "\n",
    "plt.plot(train_df_sort, \"b\", label=\"train\")\n",
    "plt.plot(test_df_sort, \"r\", label=\"test\")\n",
    "plt.xticks(rotation=\"vertical\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.index.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.index.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- So, we are training on data that came after our test data!\n",
    "- If we want to forecast, **we aren't allowed to know what happened in the future**!\n",
    "- There may be cases where this is OK, e.g. if you aren't trying to forecast and just want to understand your data (maybe you're not even splitting).\n",
    "- But, for our purposes, we want to avoid this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We'll split the data as follows:\n",
    "\n",
    "- We have total 248 data points. \n",
    "- We'll use the fist 184 data points corresponding to the first 23 days as training data - And the remaining 64 data points corresponding to the remaining 8 days as test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "citibike.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = 184\n",
    "train_df = citibike[:184]\n",
    "test_df = citibike[184:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 3))\n",
    "train_df_sort = train_df.sort_index()\n",
    "test_df_sort = test_df.sort_index()\n",
    "\n",
    "plt.plot(train_df_sort, \"b\", label=\"train\")\n",
    "plt.plot(test_df_sort, \"r\", label=\"test\")\n",
    "plt.xticks(rotation=\"vertical\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This split is looking reasonable now. We'll train our model on the blue part and evaluate it on the red part. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering for date/time columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ❓❓ Questions for you\n",
    "\n",
    "- What kind of features would you create from time series data to use in a model like a random forest or SVM?\n",
    "\n",
    "<br><br><br><br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### POSIX time feature\n",
    "- In this toy data, we just have a single feature: the date time feature. \n",
    "- We need to encode this feature if we want to build machine learning models. \n",
    "- A common way that dates are stored on computers is using POSIX time, which is the number of seconds since January 1970 00:00:00 (this is beginning of Unix time). \n",
    "- Let's start with encoding this feature as a single integer representing this POSIX time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X = (\n",
    "    citibike.index.astype(\"int64\").values.reshape(-1, 1) // 10 ** 9\n",
    ")  # convert to POSIX time by dividing by 10**9\n",
    "y = citibike.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Note \n",
    ":class: note\n",
    "In pandas, datetime objects are typically represented as Timestamp objects, which internally store time as the number of nanoseconds (1 billionth of a second. 1 second = $10^9$ nanoseconds) since the POSIX epoch (January 1, 1970, 00:00:00 UTC).\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_df.values\n",
    "y_test = test_df.values\n",
    "# convert to POSIX time by dividing by 10**9\n",
    "X_train = train_df.index.astype(\"int64\").values.reshape(-1, 1) // 10 ** 9\n",
    "X_test = test_df.index.astype(\"int64\").values.reshape(-1, 1) // 10 ** 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "X_train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "y_train[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- Our prediction task is a regression task. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Let's try random forest regression with just this feature. \n",
    "- We'll be trying out many different features. So we'll be using the function below which\n",
    "    - Splits the data \n",
    "    - Trains the given regressor model on the training data\n",
    "    - Shows train and test scores\n",
    "    - Plots the predictions on the train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code credit: Adapted from \n",
    "# https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/\n",
    "\n",
    "def eval_on_features(features, target, regressor, n_train=184, sales_data=False, \n",
    "                     ylabel='Rentals', \n",
    "                     feat_names=\"Default\", \n",
    "                     impute=True):\n",
    "    \"\"\"\n",
    "    Evaluate a regression model on a given set of features and target.\n",
    "\n",
    "    This function splits the data into training and test sets, fits the \n",
    "    regression model to the training data, and then evaluates and plots \n",
    "    the performance of the model on both the training and test datasets.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    features : array-like\n",
    "        Input features for the model.\n",
    "    target : array-like\n",
    "        Target variable for the model.\n",
    "    regressor : model object\n",
    "        A regression model instance that follows the scikit-learn API.\n",
    "    n_train : int, default=184\n",
    "        The number of samples to be used in the training set.\n",
    "    sales_data : bool, default=False\n",
    "        Indicates if the data is sales data, which affects the plot ticks.\n",
    "    ylabel : str, default='Rentals'\n",
    "        The label for the y-axis in the plot.\n",
    "    feat_names : str, default='Default'\n",
    "        Names of the features used, for display in the plot title.\n",
    "    impute : bool, default=True\n",
    "        whether SimpleImputer needs to be applied or not\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    None\n",
    "        The function does not return any value. It prints the R^2 score\n",
    "        and generates a plot.\n",
    "    \"\"\"\n",
    "\n",
    "    # Split the features and target data into training and test sets\n",
    "    X_train, X_test = features[:n_train], features[n_train:]\n",
    "    y_train, y_test = target[:n_train], target[n_train:]\n",
    "\n",
    "    if impute:\n",
    "        simp = SimpleImputer()\n",
    "        X_train = simp.fit_transform(X_train)\n",
    "        X_test = simp.transform(X_test)\n",
    "    \n",
    "    # Fit the model on the training data\n",
    "    regressor.fit(X_train, y_train)\n",
    "\n",
    "    # Print R^2 scores for training and test datasets\n",
    "    print(\"Train-set R^2: {:.2f}\".format(regressor.score(X_train, y_train)))\n",
    "    print(\"Test-set R^2: {:.2f}\".format(regressor.score(X_test, y_test)))\n",
    "\n",
    "    # Predict target variable for both training and test datasets\n",
    "    y_pred_train = regressor.predict(X_train)\n",
    "    y_pred = regressor.predict(X_test)\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(10, 3))\n",
    "\n",
    "    # If not sales data, adjust x-ticks for dates (assumes datetime format)\n",
    "    if not sales_data: \n",
    "        plt.xticks(range(0, len(X), 8), xticks.strftime(\"%a %m-%d\"), rotation=90, ha=\"left\")\n",
    "\n",
    "    # Plot training and test data, along with predictions\n",
    "    plt.plot(range(n_train), y_train, label=\"train\")\n",
    "    plt.plot(range(n_train, len(y_test) + n_train), y_test, \"-\", label=\"test\")\n",
    "    plt.plot(range(n_train), y_pred_train, \"--\", label=\"prediction train\")\n",
    "    plt.plot(range(n_train, len(y_test) + n_train), y_pred, \"--\", label=\"prediction test\")\n",
    "\n",
    "    # Set plot title, labels, and legend\n",
    "    title = regressor.__class__.__name__ + \"\\n Features= \" + feat_names\n",
    "    plt.title(title)\n",
    "    plt.legend(loc=(1.01, 0))\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(ylabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try random forest regressor with our posix time feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "regressor = RandomForestRegressor(n_estimators=100, random_state=0)\n",
    "eval_on_features(X, y, regressor, feat_names=\"POSIX time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The predictions on the training data and training score are pretty good \n",
    "- But for the test data, a constant line is predicted ...\n",
    "- What's going on?\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- The model is based on only one feature: POSIX time feature. \n",
    "- And the value of the POSIX time feature is outside the range of the feature values in the training set. \n",
    "- Tree-based models cannot _extrapolate_ to feature ranges outside the training data. \n",
    "- The model predicted the target value of the closest point in the training set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we come up with better features? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting date and time information "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note that our index is of this special type: [`DateTimeIndex`](https://pandas.pydata.org/docs/reference/api/pandas.DatetimeIndex.html). We can extract all kinds of interesting information from it.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "citibike.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "citibike.index.month_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "citibike.index.dayofweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "citibike.index.day_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "citibike.index.hour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- We noted before that the time of the day and day of the week seem quite important. \n",
    "- Let's add these two features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's first add the time of the day. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_hour = citibike.index.hour.values.reshape(-1, 1)\n",
    "X_hour[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "regressor = RandomForestRegressor(n_estimators=100, random_state=0)\n",
    "eval_on_features(X_hour, y, regressor, feat_names = \"Hour of the day\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scores are better when we add time of the day feature. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now let's add day of the week along with time of the day. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "regressor = RandomForestRegressor(n_estimators=100, random_state=0)\n",
    "X_hour_week = np.hstack(\n",
    "    [\n",
    "        citibike.index.dayofweek.values.reshape(-1, 1),\n",
    "        citibike.index.hour.values.reshape(-1, 1),\n",
    "    ]\n",
    ")\n",
    "eval_on_features(X_hour_week, y, regressor, feat_names = \"hour of day + day of week\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are much better. The time of the day and day of the week features are clearly helping. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Do we need a complex model such as a random forest? \n",
    "- Let's try `Ridge` with these features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "lr = Ridge();\n",
    "eval_on_features(X_hour_week, y, lr, feat_names = \"hour of day + day of week\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ❓❓ Questions for you\n",
    "\n",
    "- Why is `Ridge` performing poorly on the training data as well as test data? \n",
    "\n",
    "<br><br><br><br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Encoding time of day as a categorical feature\n",
    "\n",
    "- `Ridge` is performing poorly because it's not able to capture the periodic pattern.\n",
    "- The reason is that we have encoded time of day using integers. \n",
    "- A linear function can only learn a linear function of the time of day. \n",
    "- What if we encode this feature as a categorical variable? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "enc = OneHotEncoder()\n",
    "X_hour_week_onehot = enc.fit_transform(X_hour_week).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_hour_week_onehot\n",
    "X_hour_week_onehot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hour = [\"%02d:00\" % i for i in range(0, 24, 3)]\n",
    "day = [\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"]\n",
    "features = day + hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X_hour_week_onehot, columns=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_on_features(X_hour_week_onehot, y, Ridge(), feat_names=\"hour of day OHE + day of week OHE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- The scores are a bit better now. \n",
    "- What if we add interaction features (e.g., something like Day == Mon and hour == 9:00)\n",
    "- We can do it using `sklearn`'s `PolynomialFeatures` transformer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly_transformer = PolynomialFeatures(\n",
    "    interaction_only=True, include_bias=False\n",
    ")\n",
    "X_hour_week_onehot_poly = poly_transformer.fit_transform(X_hour_week_onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hour = [\"%02d:00\" % i for i in range(0, 24, 3)]\n",
    "day = [\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"]\n",
    "features = day + hour\n",
    "features_poly = poly_transformer.get_feature_names_out(features)\n",
    "features_poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hour_week_ohe_poly = pd.DataFrame(X_hour_week_onehot_poly, columns = features_poly)\n",
    "df_hour_week_ohe_poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_hour_week_onehot_poly.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = Ridge()\n",
    "eval_on_features(X_hour_week_onehot_poly, y, lr, feat_names = \"hour of day OHE + day of week OHE + interaction feats\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The scores are much better now. Since we are using a linear model, we can examine the coefficients learned by `Ridge`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "features_nonzero = np.array(features_poly)[lr.coef_ != 0]\n",
    "coef_nonzero = lr.coef_[lr.coef_ != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(coef_nonzero, index=features_nonzero, columns=[\"Coefficient\"]).sort_values(\n",
    "    \"Coefficient\", ascending=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The coefficients make sense!\n",
    "- If it's Saturday 09:00 or Wednesday 06:00, the model is likely to predict bigger number for rentals. \n",
    "- If it's Midnight or 03:00 or Sunday 06:00, the model is likely to predict smaller number for rentals. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interim summary\n",
    "\n",
    "- Success in time-series analysis heavily relies on the appropriate choice of models and features.\n",
    "- Tree-based models cannot extrapolate; caution is needed when using them with linear integer features.\n",
    "- Linear models struggle with cyclic patterns in numeric features (e.g., numerically encoded time of the day feature) because these patterns are inherently non-linear.\n",
    "- Applying one-hot encoding on such features transforms cyclic temporal features into a format where their impact on the target variable can be independently and linearly modeled, enabling linear models to effectively capture and use these cyclic patterns. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Lag-based features\n",
    "\n",
    "- So far we engineered some features and managed to get reasonable results. \n",
    "- In time series data there is temporal dependence; observations close in time tend to be correlated. \n",
    "- Currently we're using current time to predict the number of bike rentals in the next three hours. \n",
    "- But, what if the number of bike rentals is also related to bike rentals three hours ago or 6 hours ago and so on? \n",
    "  - Such features are called _lagged_ features.  \n",
    "  \n",
    "_Note: In time series analysis, you would look at something called an [autocorrelation function](https://en.wikipedia.org/wiki/Autocorrelation) (ACF), but we won't go into that here._ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's extract lag features. We can \"lag\" (or \"shift\") a time series in Pandas with the `.shift()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "citibike"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rentals_df = pd.DataFrame(citibike)\n",
    "rentals_df = rentals_df.rename(columns={\"one\":\"n_rentals\"})\n",
    "rentals_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lag_df(df, lag, cols):\n",
    "    return df.assign(\n",
    "        **{f\"{col}-{n}\": df[col].shift(n) for n in range(1, lag + 1) for col in cols}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rentals_lag5 = create_lag_df(rentals_df, 5, ['n_rentals'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rentals_lag5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note the NaN pattern. \n",
    "- We can either drop the first 5 rows or apply imputation. (You have to be careful about )\n",
    "- Let's try these features with `Ridge` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_lag_features = rentals_lag5.drop(columns = ['n_rentals']).to_numpy()\n",
    "X_lag_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = Ridge()\n",
    "eval_on_features(X_lag_features, y, lr, feat_names=\"Lag features\", impute=\"yes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are not great. But let's examine the coefficients. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_lag = rentals_lag5.drop(columns=['n_rentals']).columns.tolist()\n",
    "features_nonzero = np.array(features_lag)[lr.coef_ != 0]\n",
    "coef_nonzero = lr.coef_[lr.coef_ != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(coef_nonzero, index=features_nonzero, columns=[\"Coefficient\"]).sort_values(\n",
    "    \"Coefficient\", ascending=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about `RandomForestRegressor` model? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp = SimpleImputer()\n",
    "X_lag_features_imp = imp.fit_transform(X_lag_features)\n",
    "rf = RandomForestRegressor()\n",
    "eval_on_features(X_lag_features_imp, y, rf, feat_names=\"Lag features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are better than `Ridge` with lag features but they are not as good as the results with our previously engineered features. How about combining lag-based features and the previously extracted features? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_hour_week_onehot_poly_lag = np.hstack([X_hour_week_onehot_poly, X_lag_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor()\n",
    "eval_on_features(X_hour_week_onehot_poly_lag, y, rf, feat_names = \"hour of day OHE + day of week OHE + interaction feats + Lag feats\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some improvement but we are getting better results without the lag features in this case. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Cross-validation\n",
    "\n",
    "What about cross-validation? \n",
    "\n",
    "- We can't do regular cross-validation if we don't want to be predicting the past.\n",
    "- If you carry out regular cross-validation, as shown below, you'll be predicting the past given future which is not a realistic scenario for the deployment data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mglearn.plots.plot_cross_validation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "There is [`TimeSeriesSplit`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.TimeSeriesSplit.html) for time series data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code from sklearn documentation\n",
    "X_toy = np.array([[1, 2], [3, 4], [1, 2], [3, 4], [1, 2], [3, 4]])\n",
    "y_toy = np.array([1, 2, 3, 4, 5, 6])\n",
    "tscv = TimeSeriesSplit(n_splits=3)\n",
    "for train, test in tscv.split(X_toy):\n",
    "    print(\"%s %s\" % (train, test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's try it out with Ridge on the bike rental data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "lr = Ridge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_validate(\n",
    "    lr, X_hour_week_onehot_poly, y, cv=TimeSeriesSplit(), return_train_score=True\n",
    ")\n",
    "pd.DataFrame(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forecasting further into the future"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that we are working with the bike rentals data from August 2015. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "citibike"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is our data with lag features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rentals_lag5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's impute the data and create `X` and `y`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp = SimpleImputer()\n",
    "X_lag_features_imp = imp.fit_transform(X_lag_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rentals_lag5_X = pd.DataFrame(X_lag_features_imp, columns=rentals_lag5.drop(columns=['n_rentals']).columns)\n",
    "rentals_lag5_X.index = rentals_lag5.index\n",
    "rentals_lag5_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rentals_lag5_y = rentals_lag5['n_rentals']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's split the data and train a linear model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the given features into a training and a test set\n",
    "n_train = 184\n",
    "X_train, X_test = rentals_lag5_X[:n_train], rentals_lag5_X[n_train:]\n",
    "# also split the target array\n",
    "y_train, y_test = rentals_lag5_y[:n_train], rentals_lag5_y[n_train:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rentals_model = RandomForestRegressor(random_state=42)\n",
    "rentals_model.fit(X_train, y_train);\n",
    "print(\"Train-set R^2: {:.2f}\".format(rentals_model.score(X_train, y_train)))\n",
    "print(\"Test-set R^2: {:.2f}\".format(rentals_model.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given this, we can now predict `n_rentals` on the test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = rentals_model.predict(X_test)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_preds = X_test.assign(predicted_n_rentals=preds)\n",
    "X_test_preds = X_test_preds.assign(n_rentals=y_test)\n",
    "X_test_preds.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Ok, that is fine, but what if we want to predict 15 hours in the future? In other words, what if we want to predict number of rentals for the next three hours at time 2015-09-01 12:00:00?   \n",
    "- Well, we would not have access to our lag features!! We don't yet know `n_rentals` for\n",
    "    - 2015-09-01 00:00:00\n",
    "    - 2015-09-01 03:00:00\n",
    "    - 2015-09-01 06:00:00\n",
    "    - 2015-09-01 09:00:00"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few approaches which could be employed:\n",
    "1. Train a separate model for each number of 3-hour span. E.g. one model that predicts `n_rentals` for next three hours, another model that predicts `n_rentals` in six hours, etc. We can build these datasets.\n",
    "2. Use a multi-output model that jointly predicts `n_rentalsIn3hours`, `n_rentalsIn6hours`, etc. However, multi-output models are outside the scope of CPSC 330. \n",
    "3. Use one model and sequentially predict using a `for` loop. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If we decide to use approach 3, we would predict these values and then pretend they are true! For example, given that it's August 31st 2015 at 21:00. \n",
    "\n",
    "1. Predict `n_rentals` for September 1st 2015 at 00:00.\n",
    "2. Then to predict `n_rentals` for September 1st 2015 at 03:00, we need to know `n_rentals` for September 1st 2015 at 00:00. Use our _prediction_ for September 1st 2015 at 00:00 as the truth.\n",
    "3. Then, to predict `n_rentals` for September 1st 2015 at 06:00, we need to know `n_rentals` for September 1st 2015 at 00:00 and `n_rentals` for September 1st 2015 at 03:00. Use our predictions.\n",
    "4. Etc etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Forecasting further into the future on a retail dataset\n",
    "\n",
    "Let's consider another time series dataset, [Retail Sales of Clothing and Clothing Accessory Stores dataset](https://fred.stlouisfed.org/series/MRTSSM448USN) which is made available by the Federal Reserve Bank of St. Louis.\n",
    "\n",
    "The dataset has dates and retail sales values corresponding to the dates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retail_df = pd.read_csv(DATA_DIR + \"MRTSSM448USN.csv\", parse_dates=[\"DATE\"])\n",
    "retail_df.columns = [\"date\", \"sales\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retail_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine the min and max dates in order to split the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "retail_df[\"date\"].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retail_df[\"date\"].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm considering everything upto 01 January 2016 as training data and everything after that as test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retail_df_train = retail_df.query(\"date <= 20160101\")\n",
    "retail_df_test = retail_df.query(\"date >  20160101\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retail_df_train.plot(x=\"date\", y=\"sales\", figsize=(15, 5));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can create a dataset using purely lag features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lag_df(df, lag, cols):\n",
    "    return df.assign(\n",
    "        **{f\"{col}-{n}\": df[col].shift(n) for n in range(1, lag + 1) for col in cols}\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create lag features for the full dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retail_lag_5 = lag_df(retail_df, 5, [\"sales\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retail_train_5 = retail_lag_5.query(\"date <= 20160101\")\n",
    "retail_test_5 = retail_lag_5.query(\"date >  20160101\")\n",
    "retail_train_5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Now, if we drop the \"date\" column we have a target (\"sales\") and 5 features (the previous 5 days of sales).\n",
    "- We need to impute/drop the missing values and then we can fit a model to this. I will just drop for convenience:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retail_train_5 = retail_train_5[5:].drop(columns=[\"date\"])\n",
    "retail_train_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "retail_train_5_X = retail_train_5.drop(columns=[\"sales\"])\n",
    "retail_train_5_y = retail_train_5[\"sales\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retail_model = RandomForestRegressor(random_state=42)\n",
    "retail_model.fit(retail_train_5_X, retail_train_5_y);\n",
    "print(\"Train-set R^2: {:.2f}\".format(retail_model.score(retail_train_5_X, retail_train_5_y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given this, we can now predict the sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retail_test_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = retail_model.predict(retail_test_5.drop(columns=[\"date\", \"sales\"]))\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retail_test_5_preds = retail_test_5.assign(predicted_sales=preds)\n",
    "retail_test_5_preds.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now what if we want to predict 5 months in the future? \n",
    "- We would not have access to our features!! We don't yet know the previous months sales, or two months prior! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If we decide to use approach 3 from above, we would predict these values and then pretend they are true! For example, given that it's November 2022\n",
    "\n",
    "1. Predict December 2022 sales\n",
    "2. Then, to predict for January 2023, we need to know December 2022 sales. Use our _prediction_ for December 2022 as the truth.\n",
    "3. Then, to predict for February 2023, we need to know December 2022 and January 2023 sales. Use our predictions.\n",
    "4. Etc etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Seasonality and trends \n",
    "\n",
    "- There are some important concepts in time series that rely on having a continuous target (like we do in the retail sales example above).\n",
    "- Part of that is the idea of seasonality and trends.\n",
    "- These are mostly taken care of by our feature engineering of the data variable, but there's something important left to discuss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "retail_df_train.plot(x=\"date\", y=\"sales\", figsize=(15, 5));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It looks like there's a **trend** here - the sales are going up over time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's say we encoded the date as a feature in days like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retail_train_5_date = retail_lag_5.query(\"date <= 20160101\")\n",
    "first_day_retail = retail_train_5_date[\"date\"].min()\n",
    "\n",
    "retail_train_5_date = retail_train_5_date.assign(\n",
    "    Days_since=retail_train_5_date[\"date\"].apply(lambda x: (x - first_day_retail).days)\n",
    ")\n",
    "retail_train_5_date.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now, let's say we use all these features (the lagged version of the target and also `Days_since`.\n",
    "- If we use **linear regression** we'll learn a coefficient for `Days_since`. \n",
    "  - If that coefficient is positive, it predicts unlimited growth forever. That may not be what you want? It depends.\n",
    "- If we use a **random forest**, we'll just be doing splits from the training set, e.g. \"if `Days_since` > 9100 then do this\".\n",
    "  - There will be no splits for later time points because there is no training data there.\n",
    "  - Thus tree-based models cannot model trends.\n",
    "  - This is really important to know!!\n",
    "- Often, we model the trend separately and use the random forest to model a de-trended time series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Demo: A more complicated dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Rain in Australia](https://www.kaggle.com/jsphyg/weather-dataset-rattle-package) dataset. Predicting whether or not it will rain tomorrow based on today's measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rain_df = pd.read_csv(DATA_DIR + \"weatherAUS.csv\")\n",
    "rain_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rain_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "**Questions of interest**\n",
    "\n",
    "- Can we **forecast** into the future? Can we predict whether it's going to rain tomorrow?\n",
    "    - The target variable is `RainTomorrow`. The target is categorical and not continuous in this case. \n",
    "- Can the date/time features help us predict the target value?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "rain_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "rain_df.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- A number of missing values. \n",
    "- Some target values are also missing. I'm dropping these rows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rain_df = rain_df[rain_df[\"RainTomorrow\"].notna()]\n",
    "rain_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Parsing datetimes \n",
    "\n",
    "- In general, datetimes are a huge pain! \n",
    "    - Think of all the formats: MM-DD-YY, DD-MM-YY, YY-MM-DD, MM/DD/YY, DD/MM/YY, DD/MM/YYYY, 20:45, 8:45am, 8:45 PM, 8:45a, 08:00, 8:10:20, .......\n",
    "  - Time zones.\n",
    "  - Daylight savings...\n",
    "- Thankfully, pandas does a pretty good job here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_rain = pd.to_datetime(rain_df[\"Date\"])\n",
    "dates_rain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "They are all the same format, so we can also compare dates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_rain[1] - dates_rain[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_rain[1] > dates_rain[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(dates_rain[1] - dates_rain[0]).total_seconds()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can also easily extract information from the date columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_rain[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_rain[1].month_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_rain[1].day_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_rain[1].is_year_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_rain[1].is_leap_year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Above pandas identified the date column automatically. You can tell pandas to parse the dates when reading in the CSV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rain_df = pd.read_csv(DATA_DIR + \"weatherAUS.csv\", parse_dates=[\"Date\"])\n",
    "rain_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rain_df = rain_df[rain_df[\"RainTomorrow\"].notna()]\n",
    "rain_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rain_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rain_df.sort_values(by=\"Date\").head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rain_df.sort_values(by=\"Date\", ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ❓❓ Questions for you\n",
    "How many time series are present in this dataset? \n",
    "\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We definitely see measurements on the same day at different locations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rain_df.sort_values(by=[\"Location\", \"Date\"]).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now we gave a sequence of dates with single row per date. We have a separate timeseries for each region. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do we have equally spaced measurements? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_time_spacing_distribution(df, region=\"Adelaide\"):\n",
    "    \"\"\"\n",
    "    Plots the distribution of time spacing for a given region.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The input DataFrame with columns 'Location' and 'Date'.\n",
    "        region (str): The region (e.g., location) to analyze.\n",
    "    \"\"\"\n",
    "    # Ensure 'Date' is in datetime format\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    \n",
    "    # Filter data for the given region\n",
    "    region_data = df[df['Location'] == region]\n",
    "    \n",
    "    if region_data.empty:\n",
    "        print(f\"No data available for region: {region}\")\n",
    "        return\n",
    "    \n",
    "    # Calculate time differences\n",
    "    time_diffs = region_data['Date'].sort_values().diff().dropna()\n",
    "    \n",
    "    # Count the frequency of each time difference\n",
    "    value_counts = time_diffs.value_counts().sort_index()\n",
    "    \n",
    "    # Display value counts\n",
    "    print(f\"Time spacing counts for {region}:\\n{value_counts}\\n\")\n",
    "    \n",
    "    # Plot the bar chart\n",
    "    plt.bar(value_counts.index.astype(str), value_counts.values, color='skyblue', edgecolor='black')\n",
    "    plt.title(f\"Time Difference Distribution for {region}\")\n",
    "    plt.xlabel(\"Time Difference (days)\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_time_spacing_distribution(rain_df, 'Sydney')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The typical spacing seems to be 1 day for each location but there are several exceptions. We seem to have several missing measurements.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Train/test splits "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that we should not be calling the usual `train_test_split` with shuffling because if we want to forecast, we aren't allowed to know what happened in the future!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rain_df[\"Date\"].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rain_df[\"Date\"].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It looks like we have 10 years of data.\n",
    "- Let's use the last 2 years for test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "train_df = rain_df.query(\"Date <= 20150630\")\n",
    "test_df = rain_df.query(\"Date >  20150630\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "len(test_df) / (len(train_df) + len(test_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "As we can see, we're still using about 25% of our data as test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_sort = train_df.query(\"Location == 'Sydney'\").sort_values(by=\"Date\")\n",
    "test_df_sort = test_df.query(\"Location == 'Sydney'\").sort_values(by=\"Date\")\n",
    "\n",
    "plt.figure(figsize=(5,4))\n",
    "plt.plot(train_df_sort[\"Date\"], train_df_sort[\"Rainfall\"], \"b\", label=\"train\")\n",
    "plt.plot(test_df_sort[\"Date\"], test_df_sort[\"Rainfall\"], \"r\", label=\"test\")\n",
    "plt.xticks(rotation=\"vertical\")\n",
    "plt.legend()\n",
    "plt.ylabel(\"Rainfall (mm)\")\n",
    "plt.title(\"Train/test rainfall in Sydney\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're learning relationships from the blue part; predicting only using features in the red part from the day before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We have different types of features. So let's define a preprocessor with a column transformer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We have missing data. \n",
    "- We have categorical features and numeric features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Let's define feature types. \n",
    "- Let's start with dropping the date column and treating it as a usual supervised machine learning problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = [\n",
    "    \"MinTemp\",\n",
    "    \"MaxTemp\",\n",
    "    \"Rainfall\",\n",
    "    \"Evaporation\",\n",
    "    \"Sunshine\",\n",
    "    \"WindGustSpeed\",\n",
    "    \"WindSpeed9am\",\n",
    "    \"WindSpeed3pm\",\n",
    "    \"Humidity9am\",\n",
    "    \"Humidity3pm\",\n",
    "    \"Pressure9am\",\n",
    "    \"Pressure3pm\",\n",
    "    \"Cloud9am\",\n",
    "    \"Cloud3pm\",\n",
    "    \"Temp9am\",\n",
    "    \"Temp3pm\",\n",
    "]\n",
    "categorical_features = [\n",
    "    \"Location\",\n",
    "    \"WindGustDir\",\n",
    "    \"WindDir9am\",\n",
    "    \"WindDir3pm\",\n",
    "    \"RainToday\",\n",
    "]\n",
    "drop_features = [\"Date\"]\n",
    "target = [\"RainTomorrow\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be doing feature engineering and preprocessing features several times. So I've written a function for preprocessing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_features(\n",
    "    train_df,\n",
    "    test_df,\n",
    "    numeric_features,\n",
    "    categorical_features,\n",
    "    drop_features,\n",
    "    target\n",
    "):\n",
    "\n",
    "    all_features = set(numeric_features + categorical_features + drop_features + target)\n",
    "    if set(train_df.columns) != all_features:\n",
    "        print(\"Missing columns\", set(train_df.columns) - all_features)\n",
    "        print(\"Extra columns\", all_features - set(train_df.columns))\n",
    "        raise Exception(\"Columns do not match\")\n",
    "\n",
    "    numeric_transformer = make_pipeline(\n",
    "        SimpleImputer(strategy=\"median\"), StandardScaler()\n",
    "    )\n",
    "    categorical_transformer = make_pipeline(\n",
    "        SimpleImputer(strategy=\"constant\", fill_value=\"missing\"),\n",
    "        OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False),\n",
    "    )\n",
    "\n",
    "    preprocessor = make_column_transformer(\n",
    "        (numeric_transformer, numeric_features),\n",
    "        (categorical_transformer, categorical_features),\n",
    "        (\"drop\", drop_features),\n",
    "    )\n",
    "    preprocessor.fit(train_df)\n",
    "    ohe_feature_names = (\n",
    "        preprocessor.named_transformers_[\"pipeline-2\"]\n",
    "        .named_steps[\"onehotencoder\"]\n",
    "        .get_feature_names_out(categorical_features)\n",
    "        .tolist()\n",
    "    )\n",
    "    new_columns = numeric_features + ohe_feature_names\n",
    "\n",
    "    X_train_enc = pd.DataFrame(\n",
    "        preprocessor.transform(train_df), index=train_df.index, columns=new_columns\n",
    "    )\n",
    "    X_test_enc = pd.DataFrame(\n",
    "        preprocessor.transform(test_df), index=test_df.index, columns=new_columns\n",
    "    )\n",
    "\n",
    "    y_train = train_df[\"RainTomorrow\"]\n",
    "    y_test = test_df[\"RainTomorrow\"]\n",
    "\n",
    "    return X_train_enc, y_train, X_test_enc, y_test, preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X_train_enc, y_train, X_test_enc, y_test, preprocessor = preprocess_features(\n",
    "    train_df,\n",
    "    test_df,\n",
    "    numeric_features,\n",
    "    categorical_features,\n",
    "    drop_features, target\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_enc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### `DummyClassifier`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc = DummyClassifier()\n",
    "dc.fit(X_train_enc, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc.score(X_train_enc, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc.score(X_test_enc, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "The function below trains a logistic regression model on the train set, reports train and test scores, and returns learned coefficients as a dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def score_lr_print_coeff(preprocessor, train_df, y_train, test_df, y_test, X_train_enc):\n",
    "    lr_pipe = make_pipeline(preprocessor, LogisticRegression(max_iter=1000))\n",
    "    lr_pipe.fit(train_df, y_train)\n",
    "    print(\"Train score: {:.2f}\".format(lr_pipe.score(train_df, y_train)))\n",
    "    print(\"Test score: {:.2f}\".format(lr_pipe.score(test_df, y_test)))\n",
    "    lr_coef = pd.DataFrame(\n",
    "        data=lr_pipe.named_steps[\"logisticregression\"].coef_.flatten(),\n",
    "        index=X_train_enc.columns,\n",
    "        columns=[\"Coef\"],\n",
    "    )\n",
    "    return lr_coef.sort_values(by=\"Coef\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "score_lr_print_coeff(preprocessor, train_df, y_train, test_df, y_test, X_train_enc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Cross-validation\n",
    "\n",
    "- We can carry out cross-validation using [`TimeSeriesSplit`](https://scikit-learn.org/stable/modules/cross_validation.html#time-series-split). \n",
    "- However, things are actually more complicated here because this dataset has **multiple time series**, one per location. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.sort_values(by=[\"Date\", \"Location\"]).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- The dataframe is sorted by location and then time. \n",
    "- Our approach today will be to ignore the fact that we have multiple time series and just OHE the location\n",
    "- We'll have multiple measurements for a given timestamp, and that's OK.\n",
    "- But, `TimeSeriesSplit` expects the dataframe to be sorted by date so..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "train_df_ordered = train_df.sort_values(by=[\"Date\"])\n",
    "y_train_ordered = train_df_ordered[\"RainTomorrow\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_ordered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "lr_pipe = make_pipeline(preprocessor, LogisticRegression(max_iter=1000))\n",
    "cross_val_score(lr_pipe, train_df_ordered, y_train_ordered, cv=TimeSeriesSplit()).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Feature engineering: Encoding date/time as feature(s)\n",
    "- Can we use the `Date` to help us predict the target?\n",
    "- Probably! E.g. different amounts of rain in different seasons.\n",
    "- This is feature engineering!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Encoding time as a number\n",
    "\n",
    "- Idea 1: create a column of \"days since Nov 1, 2007\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = rain_df.query(\"Date <= 20150630\")\n",
    "test_df = rain_df.query(\"Date >  20150630\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_day = train_df[\"Date\"].min()\n",
    "\n",
    "train_df = train_df.assign(\n",
    "    Days_since=train_df[\"Date\"].apply(lambda x: (x - first_day).days)\n",
    ")\n",
    "test_df = test_df.assign(\n",
    "    Days_since=test_df[\"Date\"].apply(lambda x: (x - first_day).days)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.sort_values(by=\"Date\").head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X_train_enc, y_train, X_test_enc, y_test, preprocessor = preprocess_features(\n",
    "    train_df,\n",
    "    test_df,\n",
    "    numeric_features + [\"Days_since\"],\n",
    "    categorical_features,\n",
    "    drop_features,\n",
    "    target\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_lr_print_coeff(preprocessor, train_df, y_train, test_df, y_test, X_train_enc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Not much improvement in the scores\n",
    "- Can you think of other ways to generate features from the `Date` column? \n",
    "- What about the month - that seems relevant. How should we encode the month? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Encode it as a categorical variable?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-hot encoding of the month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = rain_df.query(\"Date <= 20150630\")\n",
    "test_df = rain_df.query(\"Date >  20150630\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.assign(\n",
    "    Month=train_df[\"Date\"].apply(lambda x: x.month_name())\n",
    ")  # x.month_name() to get the actual string\n",
    "test_df = test_df.assign(Month=test_df[\"Date\"].apply(lambda x: x.month_name()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[[\"Date\", \"Month\"]].sort_values(by=\"Month\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X_train_enc, y_train, X_test_enc, y_test, preprocessor = preprocess_features(\n",
    "    train_df, test_df, \n",
    "    numeric_features, \n",
    "    categorical_features + [\"Month\"], \n",
    "    drop_features,\n",
    "    target\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_lr_print_coeff(preprocessor, train_df, y_train, test_df, y_test, X_train_enc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No change in the results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### One-hot encoding seasons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "How about just summer/winter as a feature?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_season(month):\n",
    "    # remember this is Australia\n",
    "    WINTER_MONTHS = [\"June\", \"July\", \"August\"] \n",
    "    AUTUMN_MONTHS = [\"March\", \"April\", \"May\"]\n",
    "    SUMMER_MONTHS = [\"December\", \"January\", \"February\"]\n",
    "    SPRING_MONTHS = [\"September\", \"October\", \"November\"]\n",
    "    if month in WINTER_MONTHS:\n",
    "        return \"Winter\"\n",
    "    elif month in AUTUMN_MONTHS:\n",
    "        return \"Autumn\"\n",
    "    elif month in SUMMER_MONTHS:\n",
    "        return \"Summer\"\n",
    "    else:\n",
    "        return \"Fall\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "train_df = train_df.assign(Season=train_df[\"Month\"].apply(get_season))\n",
    "test_df = test_df.assign(Season=test_df[\"Month\"].apply(get_season))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X_train_enc, y_train, X_test_enc, y_test, preprocessor = preprocess_features(\n",
    "    train_df,\n",
    "    test_df,\n",
    "    numeric_features,\n",
    "    categorical_features + [\"Season\"],\n",
    "    drop_features + [\"Month\"],\n",
    "    target\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_enc.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff_df = score_lr_print_coeff(\n",
    "    preprocessor, train_df, y_train, test_df, y_test, X_train_enc\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "coeff_df.loc[[\"Season_Fall\", \"Season_Summer\", \"Season_Winter\", \"Season_Autumn\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- No improvements in the scores but the coefficients make some sense,\n",
    "- A negative coefficient for summer and a positive coefficients for winter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "train_df.plot(x=\"Date\", y=\"Rainfall\")\n",
    "plt.ylabel(\"Rainfall (mm)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Lag-based features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "train_df = rain_df.query(\"Date <= 20150630\")\n",
    "test_df = rain_df.query(\"Date >  20150630\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It looks like the dataframe is already sorted by Location and then by date for each Location.\n",
    "- We could have done this ourselves with: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df.sort_values(by=[\"Location\", \"Date\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But make sure to also sort the targets (i.e. do this before preprocessing)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can \"lag\" (or \"shift\") a time series in Pandas with the .shift() method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.assign(Rainfall_lag1=train_df[\"Rainfall\"].shift(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[[\"Date\", \"Location\", \"Rainfall\", \"Rainfall_lag1\"]][:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- But we have multiple time series here and we need to be more careful with this. \n",
    "- When we switch from one location to another we do not want to take the value from the previous location. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def create_lag_feature(df, orig_feature, lag):\n",
    "    \"\"\"Creates a new df with a new feature that's a lagged version of the original, where lag is an int.\"\"\"\n",
    "    # note: pandas .shift() kind of does this for you already, but oh well I already wrote this code\n",
    "\n",
    "    new_df = df.copy()\n",
    "    new_feature_name = \"%s_lag%d\" % (orig_feature, lag)\n",
    "    new_df[new_feature_name] = np.nan\n",
    "    for location, df_location in new_df.groupby(\n",
    "        \"Location\"\n",
    "    ):  # Each location is its own time series\n",
    "        new_df.loc[df_location.index[lag:], new_feature_name] = df_location.iloc[:-lag][\n",
    "            orig_feature\n",
    "        ].values\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "train_df = create_lag_feature(train_df, \"Rainfall\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[[\"Date\", \"Location\", \"Rainfall\", \"Rainfall_lag1\"]][2285:2295]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it looks good! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "- Question: is it OK to do this to the test set? Discuss.\n",
    "- It's fine if you would have this information available in deployment.\n",
    "- If we're just forecasting the next day, we should.\n",
    "- Let's include it for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "rain_df_modified = create_lag_feature(rain_df, \"Rainfall\", 1)\n",
    "train_df = rain_df_modified.query(\"Date <= 20150630\")\n",
    "test_df = rain_df_modified.query(\"Date >  20150630\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rain_df_modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X_train_enc, y_train, X_test_enc, y_test, preprocessor = preprocess_features(\n",
    "    train_df,\n",
    "    test_df,\n",
    "    numeric_features + [\"Rainfall_lag1\"],\n",
    "    categorical_features,\n",
    "    drop_features,\n",
    "    target\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_coef = score_lr_print_coeff(\n",
    "    preprocessor, train_df, y_train, test_df, y_test, X_train_enc\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_coef.loc[[\"Rainfall\", \"Rainfall_lag1\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Rainfall from today has a positive coefficient. \n",
    "- Rainfall from yesterday has a positive but a smaller coefficient. \n",
    "- If we didn't have rainfall from today feature, rainfall from yesterday feature would have received a bigger coefficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- We could also create a lagged version of the target.\n",
    "- In fact, this dataset already has that built in! `RainToday` is the lagged version of the target `RainTomorrow`.\n",
    "- We could also create lagged version of other features, or more lags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "rain_df_modified = create_lag_feature(rain_df, \"Rainfall\", 1)\n",
    "rain_df_modified = create_lag_feature(rain_df_modified, \"Rainfall\", 2)\n",
    "rain_df_modified = create_lag_feature(rain_df_modified, \"Rainfall\", 3)\n",
    "rain_df_modified = create_lag_feature(rain_df_modified, \"Humidity3pm\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rain_df_modified[\n",
    "    [\n",
    "        \"Date\",\n",
    "        \"Location\",\n",
    "        \"Rainfall\",\n",
    "        \"Rainfall_lag1\",\n",
    "        \"Rainfall_lag2\",\n",
    "        \"Rainfall_lag3\",\n",
    "        \"Humidity3pm\",\n",
    "        \"Humidity3pm_lag1\",\n",
    "    ]\n",
    "].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the pattern of `NaN` values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "train_df = rain_df_modified.query(\"Date <= 20150630\")\n",
    "test_df = rain_df_modified.query(\"Date >  20150630\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_enc, y_train, X_test_enc, y_test, preprocessor = preprocess_features(\n",
    "    train_df,\n",
    "    test_df,\n",
    "    numeric_features\n",
    "    + [\"Rainfall_lag1\", \"Rainfall_lag2\", \"Rainfall_lag3\", \"Humidity3pm_lag1\"],\n",
    "    categorical_features,\n",
    "    drop_features,\n",
    "    target\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_coef = score_lr_print_coeff(\n",
    "    preprocessor, train_df, y_train, test_df, y_test, X_train_enc\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "lr_coef.loc[\n",
    "    [\n",
    "        \"Rainfall\",\n",
    "        \"Rainfall_lag1\",\n",
    "        \"Rainfall_lag2\",\n",
    "        \"Rainfall_lag3\",\n",
    "        \"Humidity3pm\",\n",
    "        \"Humidity3pm_lag1\",\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Note the pattern in the magnitude of the coefficients. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final remarks \n",
    "\n",
    "What did we not cover?\n",
    "\n",
    "- A huge amount!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traditional time series approaches\n",
    "\n",
    "- Time series analysis is a huge field of its own \n",
    "- Traditional approaches include the [ARIMA model](https://en.wikipedia.org/wiki/Autoregressive_integrated_moving_average) and its various components/extensions.\n",
    "- In Python, the [statsmodels](https://www.statsmodels.org/) package is the place to go for this sort of thing.\n",
    "  - For example, [statsmodels.tsa.arima_model.ARIMA](https://www.statsmodels.org/stable/generated/statsmodels.tsa.arima_model.ARIMA.html).\n",
    "- These approaches can forecast, but they are also very good for understanding the temporal relationships in your data.\n",
    "- We took different route in this course, and stick to our supervised learning tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep learning \n",
    "\n",
    "- Recently, deep learning has been very successful too.\n",
    "- In particular, [recurrent neural networks](https://en.wikipedia.org/wiki/Recurrent_neural_network) (RNNs).\n",
    "  - These are not covered in CPSC 340, but I believe they are in 540 (soon to be renamed 440).\n",
    "  - [LSTMs](https://en.wikipedia.org/wiki/Long_short-term_memory) especially have shown a lot of promise in this type of task.\n",
    "  - [Here](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) is a blog post about LSTMs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of problems involving time series\n",
    "\n",
    "- A single label associated with an entire time series. \n",
    "  - We had that with images earlier on, you could have the same for a time series.\n",
    "  - E.g., for fraud detection, labelling each transaction as fraud/normal vs. labelling a person as bad/good based on their entire history.\n",
    "  - There are various approaches that can be used for this type of problem, including CNNs (Lecture 19), LSTMs, and non deep learning methods.\n",
    "- Inference problems\n",
    "  - What are the patterns in this time series?\n",
    "  - How many lags are associated with the current value?\n",
    "  - Etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unequally spaced time points\n",
    "\n",
    "- We assumed we have a measurement each day.\n",
    "- For example, when creating lag features we used consecutive rows in the DataFrame.\n",
    "- But, in fact some days were missing in this dataset.\n",
    "- More generally, what if the measurements are at arbitrary times, not equally spaced?\n",
    "  - Some of our approaches would still work, like encoding the month.\n",
    "  - Some of our approaches would not make sense, like the lags.\n",
    "  - Perhaps the measurements could be binned into equally spaced bins, or something.\n",
    "  - This is more of a hassle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other software package\n",
    "\n",
    "- A good one to know about is [Prophet](https://facebook.github.io/prophet/docs/quick_start.html).\n",
    "- [sktime](https://www.sktime.net/en/stable/get_started.html)\n",
    "    - Time series classification\n",
    "    - forecasting\n",
    "- [tslearn](https://tslearn.readthedocs.io/en/stable/)\n",
    "    - classification\n",
    "    - regression\n",
    "    - clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering\n",
    "\n",
    "- Often, a useful approach is to just _engineer your own features_.\n",
    "  - E.g., max expenditure, min expenditure, max-min, avg time gap between transactions, variance of time gap between transactions, etc etc.\n",
    "  - We could do that here as well, or in any problem.\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "conda-env-cpsc330-py",
   "language": "python",
   "name": "conda-env-cpsc330-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
